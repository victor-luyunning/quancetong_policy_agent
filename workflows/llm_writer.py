# workflows/llm_writer.py - LLMæ¶¦è‰²ç”Ÿæˆæœ€ç»ˆå›ç­”
import os
import json
import httpx
from dotenv import load_dotenv
from typing import Dict, Any

load_dotenv()

API_BASE = os.getenv("DASHSCOPE_API_BASE_URL", "")
API_KEY = os.getenv("DASHSCOPE_API_KEY", "")
CHAT_MODEL = os.getenv("DASHSCOPE_CHAT_MODEL", "qwen3-235b-a22b-instruct-2507")


async def generate_final_text(
    intent: str,
    raw_text: str,
    entities: Dict[str, Any],
    workflow_result: Dict[str, Any],
    kb_citations: str
) -> str:
    """
    ä½¿ç”¨LLMç”Ÿæˆæœ€ç»ˆç”¨æˆ·å¯è¯»çš„å›ç­”
    
    æ ¹æ®ä¸åŒæ„å›¾ä½¿ç”¨ä¸åŒçš„æ¨¡æ¿
    """
    
    # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
    context_parts = []
    
    if intent == "policy_parse":
        # æ”¿ç­–è§£ææ¨¡æ¿
        context_parts.append("## æ”¿ç­–è§£æç»“æœ")
        context_parts.append(f"æ”¿ç­–åç§°ï¼š{workflow_result.get('policy_title', 'æœªçŸ¥')}")
        context_parts.append(f"ç¦åˆ©ç±»å‹ï¼š{workflow_result.get('benefit_type', 'æœªçŸ¥')}")
        context_parts.append(f"è¡¥è´´é‡‘é¢ï¼š{workflow_result.get('benefit_amount', 'æœªçŸ¥')}")
        context_parts.append(f"é€‚ç”¨åœ°åŒºï¼š{workflow_result.get('region', 'æœªçŸ¥')}")
        context_parts.append(f"æœ‰æ•ˆæœŸï¼š{workflow_result.get('effective_period', 'æœªçŸ¥')}")
        if workflow_result.get('time_now'):
            context_parts.append(f"å½“å‰æ—¶é—´ï¼š{workflow_result['time_now']}")
        if workflow_result.get('conditions'):
            context_parts.append(f"ç”³è¯·æ¡ä»¶ï¼š{workflow_result['conditions']}")
        if workflow_result.get('procedures'):
            context_parts.append(f"åŠç†æµç¨‹ï¼š{workflow_result['procedures']}")
        if workflow_result.get('required_materials'):
            context_parts.append(f"æ‰€éœ€ææ–™ï¼š{workflow_result['required_materials']}")
        if workflow_result.get('claiming_platform'):
            context_parts.append(f"ç”³é¢†å¹³å°ï¼š{workflow_result['claiming_platform']}")
        
        # å‘½ä¸­æ”¿ç­–æ€»è§ˆï¼ˆä¸ºLLMèåˆæä¾›å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
        all_hits = workflow_result.get('all_hits', [])
        if all_hits:
            context_parts.append("\n## å‘½ä¸­æ”¿ç­–æ€»è§ˆï¼ˆæ•´åˆè¾“å‡ºï¼‰")
            for h in all_hits:
                title = h.get('title') or 'æœªçŸ¥'
                start = h.get('effective_start') or ''
                end = h.get('effective_end') or ''
                period = f"{start} è‡³ {end}".strip() if (start or end) else 'æœªçŸ¥'
                context_parts.append(
                    f"- {title}ï¼ˆæ—¶é—´ï¼š{period}ï¼Œè¡¥è´´ï¼š{h.get('benefit_amount') or 'æœªçŸ¥'}ï¼Œæ¸ é“ï¼š{h.get('claiming_platform') or 'æœªçŸ¥'}ï¼‰"
                )
        
        # ä¸»æ”¿ç­–çŠ¶æ€ï¼ˆç»“åˆæ—¶é—´MCPï¼‰
        all_hits_for_status = workflow_result.get('all_hits', [])
        if all_hits_for_status and (workflow_result.get('active_hits') or workflow_result.get('inactive_hits')):
            active_ids_s = set(workflow_result.get('active_hits', []))
            inactive_ids_s = set(workflow_result.get('inactive_hits', []))
            primary = all_hits_for_status[0]
            pid = primary.get('doc_id') or primary.get('title') or ''
            status = 'æœ‰æ•ˆ' if pid in active_ids_s else ('å·²å¤±æ•ˆ' if pid in inactive_ids_s else 'æœªçŸ¥')
            context_parts.append(f"ä¸»æ”¿ç­–çŠ¶æ€ï¼š{status}")
        
        # åˆå¹¶æ‰€æœ‰å‘½ä¸­æ”¿ç­–ï¼ˆç”¨äºLLMç»¼åˆåˆ†æï¼‰
        all_hits = workflow_result.get('all_hits', [])
        if all_hits:
            context_parts.append("\n## ç›¸å…³å‘½ä¸­æ”¿ç­–ï¼ˆä¾›ç»¼åˆå‚è€ƒï¼‰")
            # æ—¶é—´æ ¡éªŒä¿¡æ¯
            if workflow_result.get('time_now'):
                context_parts.append(f"å½“å‰æ—¶é—´ï¼š{workflow_result['time_now']}")
            active_ids = set(workflow_result.get('active_hits', []))
            inactive_ids = set(workflow_result.get('inactive_hits', []))
            for h in all_hits:
                period = ""
                if h.get('effective_start') or h.get('effective_end'):
                    start = h.get('effective_start') or ''
                    end = h.get('effective_end') or ''
                    period = f"{start} è‡³ {end}".strip()
                hid = h.get('doc_id') or h.get('title') or ''
                status = "æœ‰æ•ˆ" if hid in active_ids else ("å·²å¤±æ•ˆ" if hid in inactive_ids else "æœªçŸ¥")
                context_parts.append(
                    f"- æ ‡é¢˜ï¼š{h.get('title') or 'æœªçŸ¥'}ï¼›æ—¶é—´ï¼š{period or 'æœªçŸ¥'}ï¼›çŠ¶æ€ï¼š{status}ï¼›è¡¥è´´ï¼š{h.get('benefit_amount') or 'æœªçŸ¥'}ï¼›æ¸ é“ï¼š{h.get('claiming_platform') or 'æœªçŸ¥'}ï¼›æµç¨‹ï¼š{h.get('procedures') or 'æœªçŸ¥'}ï¼›åœ°åŒºï¼š{(h.get('region_city') or '')} {h.get('region_province') or ''}".strip()
                )
    
    elif intent == "personal_welfare":
        # ç¦åˆ©è®¡ç®—æ¨¡æ¿
        context_parts.append("## ç¦åˆ©è®¡ç®—ç»“æœ")
        context_parts.append(f"æ‚¨å¯è·å¾—è¡¥è´´ï¼š{workflow_result.get('subsidy_amount', 0)}å…ƒ")
        context_parts.append(f"è¡¥è´´æ˜ç»†ï¼š{workflow_result.get('subsidy_breakdown', 'æ— ')}")
        if workflow_result.get('constraints'):
            context_parts.append(f"é™åˆ¶æ¡ä»¶ï¼š{workflow_result['constraints']}")
        if workflow_result.get('required_materials'):
            context_parts.append(f"æ‰€éœ€ææ–™ï¼š{workflow_result['required_materials']}")
        if workflow_result.get('claiming_platform'):
            context_parts.append(f"ç”³é¢†å¹³å°ï¼š{workflow_result['claiming_platform']}")
    
    elif intent == "regional_compare":
        # åŒºåŸŸå¯¹æ¯”æ¨¡æ¿
        context_parts.append("## åŒºåŸŸæ”¿ç­–å¯¹æ¯”")
        context_parts.append(f"å¯¹æ¯”åœ°åŒºï¼š{', '.join(workflow_result.get('regions_compared', []))}")
        context_parts.append(f"å¯¹æ¯”æ€»ç»“ï¼š{workflow_result.get('summary', 'æ— ')}")
        
        # å¯¹æ¯”è¡¨æ ¼
        comparison_table = workflow_result.get('comparison_table', [])
        if comparison_table:
            context_parts.append("\nè¯¦ç»†å¯¹æ¯”ï¼š")
            for item in comparison_table:
                context_parts.append(f"- {item['region']}ï¼š{item['benefit_amount']}")
    
    elif intent == "investment_signal":
        # ä¼ä¸šæŠ•èµ„ä¿¡å·ç¯æ¨¡æ¿
        context_parts.append("## ä¼ä¸šæŠ•èµ„åˆ†æ")
        context_parts.append(f"æŠ•èµ„ä¿¡å·ï¼š{workflow_result.get('investment_signal', 'é»„ç¯')}")
        context_parts.append(f"è¡Œä¸šæ¦‚å†µï¼š{workflow_result.get('industry_summary', 'æ— ')}")
        
        # æ¨èä¼ä¸š
        recommended = workflow_result.get('recommended_companies', [])
        if recommended:
            context_parts.append("\næ¨èä¼ä¸šTop 5ï¼š")
            for comp in recommended[:5]:
                context_parts.append(
                    f"- {comp['company_name']}ï¼ˆ{comp['location']}ï¼‰"
                    f"ï¼Œè¯„åˆ†ï¼š{comp['total_score']}ï¼Œæ‰©å±•æ„æ„¿ï¼š{comp['expansion_willingness']}"
                )
    
    context = "\n".join(context_parts)
    
    # æ„å»ºLLM Prompt
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ”¿ç­–å’¨è¯¢æ™ºèƒ½ä½“çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å—ã€‚ä½ éœ€è¦æ ¹æ®ç»“æ„åŒ–æ•°æ®ï¼Œç”Ÿæˆå‹å¥½ã€ä¸“ä¸šã€æ˜“æ‡‚ä¸”â€œäººæ€§åŒ–â€çš„ä¸­æ–‡å›ç­”ã€‚

è¾“å‡ºé£æ ¼è¦æ±‚ï¼š
1. ç”¨è‡ªç„¶æ®µè½è¡¨è¾¾ï¼Œå…ˆç»™ç»“è®ºå†è§£é‡Šï¼Œä¸è¦è¾“å‡ºæ ‡é¢˜ã€åˆ—è¡¨ç¬¦å·æˆ–è¡¨æ ¼ï¼›ä¸è¦å¤è¿°ä¸Šä¸‹æ–‡ä¸­çš„æç¤ºè¯­ï¼ˆå¦‚â€œæ”¿ç­–è§£æç»“æœ/å‘½ä¸­æ”¿ç­–æ€»è§ˆ/ç›¸å…³å‘½ä¸­æ”¿ç­–â€ç­‰ï¼‰ã€‚
2. èåˆå¤šæ¡æˆ–å¤šè½®æ”¿ç­–æ—¶ï¼ŒæŒ‰æ—¶é—´é¡ºåºç®€æ´å½’çº³ï¼ˆä¾‹å¦‚ï¼šä¸ŠåŠå¹´â€¦ï¼›5-6æœˆé¦–ä¿â€¦ï¼›7-9æœˆä¸‰è½®â€¦ï¼‰ï¼Œé¿å…é€æ¡ç¡¬åˆ—å‡ºã€‚
3. é‡ç‚¹çªå‡ºå…³é”®ä¿¡æ¯ï¼šå½“å‰çŠ¶æ€ï¼ˆç»“åˆå½“å‰æ—¶é—´ä¸æœ‰æ•ˆ/å¤±æ•ˆï¼‰ã€ç”³é¢†æ¸ é“ã€æ‰€éœ€ææ–™ã€åŠç†æµç¨‹ã€é‡‘é¢æˆ–æ¡£ä½ï¼ˆå¦‚æœªæ˜ç¡®è¯·è¯´æ˜â€œå°šæœªæ˜ç¡®â€ï¼‰ã€‚
4. ä¸¥ç¦ç¼–é€ ï¼šä»…åŸºäºç»“æ„åŒ–æ•°æ®å†…å®¹ï¼›å¦‚ä¿¡æ¯ç¼ºå¤±è¯·ä»¥â€œå°šæœªæ˜ç¡®â€æˆ–â€œä»¥å®˜æ–¹å…¬å‘Šä¸ºå‡†â€è¡¨è¿°ã€‚
5. å­—æ•°æ§åˆ¶åœ¨200-350å­—ï¼Œè¯­è¨€å‹å¥½ã€ç®€æ´ï¼Œä¸ä½¿ç”¨æŠ€æœ¯åŒ–æªè¾ã€‚
6. è‹¥ç»“æ„åŒ–æ•°æ®æç¤ºâ€œä¸»æ”¿ç­–çŠ¶æ€ï¼šå·²å¤±æ•ˆæˆ–æœªçŸ¥â€ï¼Œä¸å¾—ä½¿ç”¨â€œä»åœ¨æœ‰æ•ˆæœŸå†…/æ­£åœ¨å®æ–½â€ç­‰æªè¾ï¼Œéœ€æ˜ç¡®æé†’æ”¿ç­–å·²æˆªæ­¢æˆ–æ— æ³•ç¡®è®¤ã€‚
7. è‹¥å­˜åœ¨å¤šè½®æ¬¡æˆ–å¤šæ¡å‘½ä¸­ï¼Œå¿…é¡»æ•´åˆè¾“å‡ºï¼Œç¦æ­¢ä»…é€‰å–ç¬¬ä¸€æ¡ã€‚
8. å½“å½“å‰æ—¶é—´æ™šäºæ”¿ç­–æœ‰æ•ˆæœŸç»“æŸæ—¶é—´æ—¶ï¼Œéœ€æ˜ç¡®æ ‡æ³¨â€œå·²æˆªæ­¢/å†å²æ´»åŠ¨â€ï¼Œé¿å…ä½¿ç”¨â€œæ­£åœ¨å®æ–½/ä»åœ¨æœ‰æ•ˆæœŸå†…â€çš„è¡¨è¿°ã€‚
9. ç¦æ­¢è¾“å‡ºQuickChartæˆ–å…¶ä»–ç¬¬ä¸‰æ–¹å›¾è¡¨æœåŠ¡çš„URLé“¾æ¥ï¼Œå›¾è¡¨å°†ç”±ç³»ç»Ÿè‡ªåŠ¨å¤„ç†å¹¶å±•ç¤ºã€‚"""

    user_prompt = f"""ç”¨æˆ·æŸ¥è¯¢ï¼š{raw_text}

ç»“æ„åŒ–æ•°æ®ï¼ˆä»…ä¾›å‚è€ƒï¼Œä¸è¦åŸæ ·å¤åˆ¶ï¼‰ï¼š
{context}

ç”Ÿæˆè¦æ±‚ï¼š
- è¯·è¾“å‡º1-3æ®µè‡ªç„¶è¯­è¨€è¯´æ˜ï¼Œä¸è¦å‡ºç°â€œæ”¿ç­–è§£æç»“æœ/å‘½ä¸­æ”¿ç­–æ€»è§ˆ/ç›¸å…³å‘½ä¸­æ”¿ç­–â€ç­‰ä¸Šä¸‹æ–‡æ ‡é¢˜ï¼Œä¹Ÿä¸è¦ç”¨é¡¹ç›®ç¬¦å·æˆ–è¡¨æ ¼ã€‚
- è‹¥å­˜åœ¨å¤šè½®æ¬¡æˆ–å¤šæ¡æ”¿ç­–å‘½ä¸­ï¼Œè¯·æ•´åˆå¹¶æŒ‰æ—¶é—´é¡ºåºç®€æ´å½’çº³ï¼Œç¦æ­¢ä»…å–ç¬¬ä¸€æ¡ã€‚
- ä¸¥æ ¼ä¾æ®æä¾›çš„â€œå½“å‰æ—¶é—´â€å’Œâ€œçŠ¶æ€ï¼ˆæœ‰æ•ˆ/å·²å¤±æ•ˆ/æœªçŸ¥ï¼‰â€è¿›è¡Œè¡¨è¿°ï¼›è‹¥å¤±æ•ˆæˆ–æœªçŸ¥ï¼Œç¦æ­¢ä½¿ç”¨â€œæ­£åœ¨å®æ–½/ä»åœ¨æœ‰æ•ˆæœŸå†…â€ã€‚
- æ˜ç¡®ç”³è¯·æ¸ é“ã€æ‰€éœ€ææ–™å’Œæµç¨‹ï¼Œé‡‘é¢æˆ–æ¡£ä½å¦‚æœªæ˜ç¡®è¯·è¯´æ˜â€œå°šæœªæ˜ç¡®â€ã€‚
- ä¸è¦ç¼–é€ è¶…å‡ºç»“æ„åŒ–æ•°æ®çš„ä¿¡æ¯ã€‚

è¯·ç”Ÿæˆå‹å¥½çš„å›ç­”ï¼š"""
    
    try:
        headers = {
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json"
        }
        async with httpx.AsyncClient(timeout=30) as client:
            content = ""
            ok = False
            # ä¼˜å…ˆå°è¯• OpenAI å…¼å®¹æ¨¡å¼
            if API_BASE and ("compatible-mode" in API_BASE):
                url1 = f"{API_BASE}/chat/completions"
                payload1 = {
                    "model": CHAT_MODEL,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.7,
                    "max_tokens": 500
                }
                try:
                    r1 = await client.post(url1, headers=headers, json=payload1)
                    if r1.status_code == 200:
                        data = r1.json()
                        content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                        ok = True
                except Exception:
                    ok = False
            # å…¼å®¹æ¨¡å¼ä¸å¯ç”¨æˆ–å¤±è´¥æ—¶ï¼Œå›é€€åˆ° DashScope åŸç”Ÿæ¥å£
            if not ok:
                base2 = API_BASE or "https://dashscope.aliyuncs.com"
                url2 = f"{base2}/v1/services/aigc/text-generation/generation"
                payload2 = {
                    "model": CHAT_MODEL,
                    "input": {
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ]
                    },
                    "parameters": {
                        "temperature": 0.7,
                        "max_tokens": 500
                    }
                }
                r2 = await client.post(url2, headers=headers, json=payload2)
                r2.raise_for_status()
                data = r2.json()
                # å…¼å®¹å¤šç§è¿”å›ç»“æ„
                content = (
                    data.get("output", {}).get("text") or
                    data.get("choices", [{}])[0].get("message", {}).get("content", "") or
                    json.dumps(data, ensure_ascii=False)
                )
            if kb_citations:
                content += f"\n\nğŸ“š å‚è€ƒæ¥æºï¼š{kb_citations.replace('|', ', ')}"
            return content.strip()
            
    except Exception as e:
        print(f"[LLM Writer] è°ƒç”¨å¤±è´¥: {e}")
        # é™çº§ï¼šç›´æ¥è¿”å›ç»“æ„åŒ–æ•°æ®
        fallback = context
        if kb_citations:
            fallback += f"\n\nå‚è€ƒæ¥æºï¼š{kb_citations}"
        return fallback
