#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹æ¯”æµ‹è¯• DeepSeek-V3 å’Œ Qwen-Max ä½¿ç”¨MCPè”ç½‘æœç´¢å¢å¼ºçš„è¡¨ç°
"""

import os
import json
import csv
import time
from datetime import datetime
import requests
from openai import OpenAI

# ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
DASHSCOPE_API_BASE_URL = os.getenv("DASHSCOPE_API_BASE_URL")

# æ”¿ç­–çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„
KNOWLEDGE_BASE_PATH = "data/policies/æ€»çŸ¥è¯†åº“.md"

def load_local_knowledge():
    """åŠ è½½æœ¬åœ°æ”¿ç­–çŸ¥è¯†åº“"""
    try:
        with open(KNOWLEDGE_BASE_PATH, 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except Exception as e:
        print(f"  âš ï¸ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
        return ""

def extract_relevant_knowledge(question, knowledge_base, max_length=2000):
    """ä»çŸ¥è¯†åº“ä¸­æå–ç›¸å…³å†…å®¹ï¼ˆç®€å•å…³é”®è¯åŒ¹é…ï¼‰"""
    import re
    
    # æå–é—®é¢˜ä¸­çš„å…³é”®è¯
    keywords = re.findall(r'[\u4e00-\u9fa5]{2,}', question)
    
    # åˆ†æ®µçŸ¥è¯†åº“
    sections = knowledge_base.split('\n\n')
    
    # æ‰¾å‡ºåŒ…å«å…³é”®è¯çš„æ®µè½
    relevant_sections = []
    for section in sections:
        score = sum(1 for keyword in keywords if keyword in section)
        if score > 0:
            relevant_sections.append((score, section))
    
    # æŒ‰ç›¸å…³åº¦æ’åº
    relevant_sections.sort(reverse=True, key=lambda x: x[0])
    
    # æ‹¼æ¥æœ€ç›¸å…³çš„å†…å®¹
    result = ""
    for score, section in relevant_sections[:5]:  # å–å‰5ä¸ªæœ€ç›¸å…³çš„æ®µè½
        if len(result) + len(section) > max_length:
            break
        result += section + "\n\n"
    
    return result.strip()

def load_test_questions(json_file="test_questions_dataset.json"):
    """åŠ è½½æµ‹è¯•é—®é¢˜"""
    with open(json_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def test_with_knowledge_base(question, model_name, api_key=DASHSCOPE_API_KEY):
    """ä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“å¢å¼ºæµ‹è¯•æ¨¡å‹"""
    try:
        # 1. ç›´æ¥åŠ è½½æ•´ä»½çŸ¥è¯†åº“ï¼ˆä¸åšä»»ä½•æ–‡æœ¬å¤„ç†ï¼‰
        print(f"    ğŸ“š ä»æœ¬åœ°çŸ¥è¯†åº“åŠ è½½å…¨æ–‡...")
        
        # é¦–æ¬¡è°ƒç”¨æ—¶åŠ è½½çŸ¥è¯†åº“
        if not hasattr(test_with_knowledge_base, 'knowledge_base'):
            test_with_knowledge_base.knowledge_base = load_local_knowledge()
        
        knowledge_base = test_with_knowledge_base.knowledge_base
        
        if knowledge_base:
            print(f"    âœ… çŸ¥è¯†åº“å·²åŠ è½½ï¼Œé•¿åº¦ {len(knowledge_base)} å­—ç¬¦")
            # ä¸åšä»»ä½•æˆªæ–­ã€åˆ‡åˆ†ã€å…³é”®è¯æŠ½å–ï¼Œç›´æ¥ä½œä¸ºä¸Šä¸‹æ–‡ä¼ ç»™æ¨¡å‹
            context = "\n\nä»¥ä¸‹æ˜¯æ”¿ç­–çŸ¥è¯†åº“å…¨æ–‡ï¼š\n" + knowledge_base
        else:
            print(f"    âš ï¸ çŸ¥è¯†åº“ä¸ºç©º")
            context = ""
        
        enhanced_question = f"""{context}

åŸºäºä»¥ä¸Šæ”¿ç­–ä¿¡æ¯ï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
{question}

è¯·ç»™å‡ºå‡†ç¡®ç®€æ´çš„ç­”æ¡ˆï¼Œé‡ç‚¹å…³æ³¨å…·ä½“çš„é‡‘é¢ã€æ—¶é—´ã€æ¡£ä½ç­‰å…³é”®ä¿¡æ¯ã€‚"""
        
        # 3. è°ƒç”¨LLM
        client = OpenAI(
            api_key=api_key,
            base_url=DASHSCOPE_API_BASE_URL
        )
        
        start_time = time.time()
        
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ”¿ç­–å’¨è¯¢åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·æŸ¥è¯¢å±±ä¸œçœå’Œæµå—å¸‚çš„æ¶ˆè´¹è¡¥è´´æ”¿ç­–ã€‚è¯·åŸºäºæä¾›çš„ä¿¡æ¯ç»™å‡ºå‡†ç¡®çš„ç­”æ¡ˆã€‚"
                },
                {
                    "role": "user",
                    "content": enhanced_question
                }
            ],
            temperature=0.3,
            max_tokens=1500
        )
        
        response_time = time.time() - start_time
        
        return {
            "status": "success",
            "answer": response.choices[0].message.content,
            "response_time": response_time,
            "model": model_name,
            "knowledge_used": len(context) > 0,
            "error": ""
        }
    
    except Exception as e:
        return {
            "status": "error",
            "answer": "",
            "response_time": 0,
            "model": model_name,
            "knowledge_used": False,
            "error": str(e)
        }

def evaluate_answer(expected, actual):
    """ç®€å•è¯„ä¼°ç­”æ¡ˆï¼ˆå…³é”®è¯åŒ¹é…ï¼‰"""
    if not actual:
        return "é”™è¯¯", "ç­”æ¡ˆä¸ºç©º"
    
    # æå–å…³é”®æ•°å­—å’Œå…³é”®è¯
    import re
    expected_nums = set(re.findall(r'\d+', expected))
    actual_nums = set(re.findall(r'\d+', actual))
    
    expected_keywords = set(re.findall(r'[\u4e00-\u9fa5]{2,}', expected))
    actual_keywords = set(re.findall(r'[\u4e00-\u9fa5]{2,}', actual))
    
    # æ•°å­—åŒ¹é…åº¦
    num_match = len(expected_nums & actual_nums) / len(expected_nums) if expected_nums else 0
    
    # å…³é”®è¯åŒ¹é…åº¦
    keyword_match = len(expected_keywords & actual_keywords) / len(expected_keywords) if expected_keywords else 0
    
    overall_score = (num_match + keyword_match) / 2
    
    if overall_score >= 0.7:
        return "æ­£ç¡®", f"åŒ¹é…åº¦{overall_score*100:.0f}%"
    elif overall_score >= 0.4:
        return "éƒ¨åˆ†æ­£ç¡®", f"åŒ¹é…åº¦{overall_score*100:.0f}%"
    else:
        return "é”™è¯¯", f"åŒ¹é…åº¦ä½({overall_score*100:.0f}%)"

def main():
    print("="*80)
    print("ğŸ”¬ Moonshot-Kimi-K2-Instruct + æœ¬åœ°çŸ¥è¯†åº“æµ‹è¯•")
    print("="*80)
    print(f"æ¨¡å‹: Moonshot-Kimi-K2-Instruct")
    print(f"å¢å¼ºæ–¹å¼: æœ¬åœ°æ”¿ç­–çŸ¥è¯†åº“")
    print(f"çŸ¥è¯†åº“è·¯å¾„: {KNOWLEDGE_BASE_PATH}")
    print(f"API Key: {'å·²é…ç½®' if DASHSCOPE_API_KEY else 'æœªé…ç½®'}")
    print("="*80 + "\n")
    
    # åŠ è½½æµ‹è¯•é—®é¢˜
    questions = load_test_questions()
    print(f"ğŸ“ åŠ è½½äº† {len(questions)} ä¸ªæµ‹è¯•é—®é¢˜\n")
    
    # å‡†å¤‡ç»“æœå­˜å‚¨
    results = []
    
    # å¯¹æ¯ä¸ªé—®é¢˜è¿›è¡Œæµ‹è¯•
    for i, q in enumerate(questions, 1):
        print(f"\n[{i}/{len(questions)}] æµ‹è¯•é—®é¢˜ #{q['id']}")
        print(f"é—®é¢˜: {q['question']}")
        print(f"é¢„æœŸ: {q['expected_answer'][:50]}...")
        
        # æµ‹è¯• GPT-4 Turboï¼ˆå•æ¬¡ï¼‰
        print(f"\n  ğŸ”µ GPT-4 Turbo + çŸ¥è¯†åº“...")
        gpt4_result = test_with_knowledge_base(q['question'], "Moonshot-Kimi-K2-Instruct")
        if gpt4_result['status'] == 'success':
            kb_status = "ä½¿ç”¨çŸ¥è¯†åº“" if gpt4_result['knowledge_used'] else "æ— çŸ¥è¯†åº“"
            print(f"  âœ… å®Œæˆ ({gpt4_result['response_time']:.2f}ç§’, {kb_status})")
            gpt4_eval, gpt4_reason = evaluate_answer(q['expected_answer'], gpt4_result['answer'])
            print(f"  è¯„ä¼°: {gpt4_eval} - {gpt4_reason}")
        else:
            print(f"  âŒ å¤±è´¥: {gpt4_result['error']}")
            gpt4_eval, gpt4_reason = "é”™è¯¯", gpt4_result['error']
        
        # ä¿å­˜ç»“æœ
        results.append({
            'id': q['id'],
            'question': q['question'],
            'expected_answer': q['expected_answer'],
            'category': q['category'],
            'campaign_id': q['campaign_id'],
            
            'gpt4_answer': gpt4_result['answer'],
            'gpt4_time': gpt4_result['response_time'],
            'gpt4_status': gpt4_result['status'],
            'gpt4_knowledge_used': gpt4_result['knowledge_used'],
            'gpt4_evaluation': gpt4_eval,
            'gpt4_reason': gpt4_reason,
            'gpt4_error': gpt4_result['error']
        })
        
        # çŸ­æš‚å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
        time.sleep(2)
    
    # ä¿å­˜ç»“æœåˆ°CSV
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = "model_evaluate"
    os.makedirs(output_dir, exist_ok=True)
    output_csv = os.path.join(output_dir, f"mcp_comparison_results_{timestamp}.csv")
    
    fieldnames = [
        'id', 'question', 'expected_answer', 'category', 'campaign_id',
        'gpt4_answer', 'gpt4_time', 'gpt4_status', 'gpt4_knowledge_used',
        'gpt4_evaluation', 'gpt4_reason', 'gpt4_error'
    ]
    
    with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)
    
    print(f"\n\nâœ… æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_csv}")
    
    # ç»Ÿè®¡
    gpt4_correct = sum(1 for r in results if r['gpt4_evaluation'] in ['æ­£ç¡®', 'éƒ¨åˆ†æ­£ç¡®'])
    gpt4_success = [r for r in results if r['gpt4_status'] == 'success']
    gpt4_avg_time = sum(r['gpt4_time'] for r in gpt4_success) / len(gpt4_success) if gpt4_success else 0
    
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"  å‡†ç¡®ç‡: {gpt4_correct}/{len(results)} ({gpt4_correct/len(results)*100:.1f}%)")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {gpt4_avg_time:.2f}ç§’")
    print(f"  æˆåŠŸç‡: {len(gpt4_success)}/{len(results)}")

if __name__ == "__main__":
    main()
